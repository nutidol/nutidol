{"version":3,"file":"component---src-pages-p-sg-js-ce1c2fde56eb866652f7.js","mappings":"uNAEIA,EAAY,CAAC,WAAY,YAAa,QAAS,UAAW,gBAAiB,aAqC3EC,GA5BQ,WAKH,SAKE,SAKM,SAKJ,SAQY,cAAiB,SAAUC,EAAMC,GACxD,IAAIC,EAAWF,EAAKE,SAChBC,EAAYH,EAAKG,UACjBC,EAAQJ,EAAKI,MACbC,EAAUL,EAAKK,QACfC,EAAgBN,EAAKM,cACrBC,EAAYP,EAAKO,UACjBC,GAAQ,OAA8BR,EAAMF,GAEhDI,GAAW,QAAmBA,EAAU,OACxC,IAAIO,EAAU,IAAWL,GAASF,EAAW,SAAUG,GAAW,UAAWC,GAAiB,iBAAkBC,GAAaL,EAAW,cACxI,OAAoB,gBAAoB,OAAO,OAAS,CAEtDD,IAAKA,GACJO,EAAO,CACRL,UAAW,IAAWA,EAAWM,UAGrCV,EAAMW,YAAc,QACpBX,EAAMY,aAzBa,CACjBP,OAAO,EACPC,SAAS,EACTC,eAAe,EACfC,WAAW,GAsBb,O,oGC3DA,EAAe,IAA0B,kDCAzC,EAAe,IAA0B,kDCAzC,EAAe,IAA0B,8D,8BC0JzC,EA9IqB,kBACnB,2BACE,gBAACK,EAAA,EAAD,MACA,uBACET,UAAU,YACVU,MAAO,CACLC,OAAO,SACPC,SAAU,IACVC,cAAc,UAGhB,sBAAIb,UAAU,yBAAd,2DAGA,qBAAGA,UAAU,8BAAb,wBACA,qBAAGA,UAAU,8BAAb,oFAIA,sBAAIA,UAAU,QAAd,aACA,y1CAuBA,sBAAIA,UAAU,QAAd,6DAGA,gBAACJ,EAAA,EAAD,CAAOkB,IAAKC,EAAiBf,UAAU,qBAAqBgB,IAAI,KAAKf,OAAK,IAC1E,sBAAID,UAAU,aAAd,wBACA,qBAAGA,UAAU,QAAb,+tBAaA,sBAAIA,UAAU,aAAd,uBACA,qBAAGA,UAAU,QAAb,2CAC2C,IACzC,gBAAC,EAAAiB,KAAD,CAAMC,GAAG,0BAAT,WAFF,iGAGyE,IACvE,gBAAC,EAAAD,KAAD,CAAMC,GAAG,0BAAT,UAJF,4mBAgBA,uBAAKlB,UAAU,QACb,gBAACmB,EAAA,EAAD,CAAUC,WAAS,EAACC,WAAS,EAACC,SAAS,QACrC,gBAACH,EAAA,OAAD,KACE,uBAAKnB,UAAU,iBAAiBc,IAAKS,EAAKP,IAAI,cAAcf,OAAK,KAEnE,gBAACkB,EAAA,OAAD,KACE,uBACEnB,UAAU,iBACVc,IAAKU,EACLR,IAAI,eACJf,OAAK,OAMb,sBAAID,UAAU,QAAd,UACA,unCAkBA,sBAAIA,UAAU,QAAd,gBACA,+wCAqBA,6BAGF,gBAACyB,EAAA,EAAD","sources":["webpack://gatsby-starter-default/./node_modules/react-bootstrap/esm/Image.js","webpack://gatsby-starter-default/./src/images/sg/tb1.jpg","webpack://gatsby-starter-default/./src/images/sg/tb2.jpg","webpack://gatsby-starter-default/./src/images/sg/userstudyimages.jpg","webpack://gatsby-starter-default/./src/pages/p-sg.js"],"sourcesContent":["import _extends from \"@babel/runtime/helpers/esm/extends\";\nimport _objectWithoutPropertiesLoose from \"@babel/runtime/helpers/esm/objectWithoutPropertiesLoose\";\nvar _excluded = [\"bsPrefix\", \"className\", \"fluid\", \"rounded\", \"roundedCircle\", \"thumbnail\"];\nimport classNames from 'classnames';\nimport React from 'react';\nimport PropTypes from 'prop-types';\nimport { useBootstrapPrefix } from './ThemeProvider';\nexport var propTypes = {\n  /**\n   * @default 'img'\n   */\n  bsPrefix: PropTypes.string,\n\n  /**\n   * Sets image as fluid image.\n   */\n  fluid: PropTypes.bool,\n\n  /**\n   * Sets image shape as rounded.\n   */\n  rounded: PropTypes.bool,\n\n  /**\n   * Sets image shape as circle.\n   */\n  roundedCircle: PropTypes.bool,\n\n  /**\n   * Sets image shape as thumbnail.\n   */\n  thumbnail: PropTypes.bool\n};\nvar defaultProps = {\n  fluid: false,\n  rounded: false,\n  roundedCircle: false,\n  thumbnail: false\n};\nvar Image = /*#__PURE__*/React.forwardRef(function (_ref, ref) {\n  var bsPrefix = _ref.bsPrefix,\n      className = _ref.className,\n      fluid = _ref.fluid,\n      rounded = _ref.rounded,\n      roundedCircle = _ref.roundedCircle,\n      thumbnail = _ref.thumbnail,\n      props = _objectWithoutPropertiesLoose(_ref, _excluded);\n\n  bsPrefix = useBootstrapPrefix(bsPrefix, 'img');\n  var classes = classNames(fluid && bsPrefix + \"-fluid\", rounded && \"rounded\", roundedCircle && \"rounded-circle\", thumbnail && bsPrefix + \"-thumbnail\");\n  return /*#__PURE__*/React.createElement(\"img\", _extends({\n    // eslint-disable-line jsx-a11y/alt-text\n    ref: ref\n  }, props, {\n    className: classNames(className, classes)\n  }));\n});\nImage.displayName = 'Image';\nImage.defaultProps = defaultProps;\nexport default Image;","export default __webpack_public_path__ + \"static/tb1-f04a936d3d9eb6e057a11b663aebcaed.jpg\";","export default __webpack_public_path__ + \"static/tb2-260847acc6d9d1428996ae505f029151.jpg\";","export default __webpack_public_path__ + \"static/userstudyimages-22a03569a4ebfbe4344a3c9142983c25.jpg\";","import * as React from \"react\"\nimport Header from \"../components/Header\"\nimport Footer from \"../components/Footer\"\nimport tb1 from \"../images/sg/tb1.jpg\"\nimport tb2 from \"../images/sg/tb2.jpg\"\nimport userstudyimages from \"../images/sg/userstudyimages.jpg\"\n\n\n\nimport { Image, Carousel } from \"react-bootstrap\"\nimport { Link } from \"gatsby\"\n\nconst cookBurnPage = () => (\n  <div>\n    <Header></Header>\n    <div\n      className=\"container\"\n      style={{\n        margin: `0 auto`,\n        maxWidth: 960,\n        paddingBottom: `200px`,\n      }}\n    >\n      <h2 className=\"text-center pt-4 pb-2\">\n        Visualising and Evaluating Outputs of Generative Models\n      </h2>\n      <p className=\"text-center text-secondary\">Jan 2022 ~ Sept 2022</p>\n      <p className=\"text-center text-secondary\">\n        Which metrics are the best to evaluate outputs of generative models like\n        humans?\n      </p>\n      <h3 className=\"pb-2\">Abstract </h3>\n      <p>\n        These days, computers can generate various fake images which look just\n        like real images. Generative Adversarial Networks (GANs) are known to be\n        the techniques for generating realistic images. Since the generative\n        model does not have a loss function, like other deep learning models,\n        the methods that suitably evaluate the models are attractive to search.\n        Many ways are introduced to assess the output of a generative model,\n        such as calculating latent space and human eyes. This project aims to\n        evaluate and understand the synthetic images from GANs by computing\n        famous quantitative metrics scores and collecting quantitative scores\n        from human perception through surveys. Lastly, the results from this\n        project could be a valuable tool for increasing the number of many new\n        and exciting generative models. According to this project, the study\n        metric close to the human perspective is the Precision metric, as its\n        score for StyleGAN2 is the best, and StyleGAN2â€™s outputs are chosen to\n        be the most realistic images from the user study. Furthermore, there is\n        an interesting fact about images from StyleGAN-XL. While it produces the\n        best FID and KID scores, it is ranked worst from the human perspective.\n        This contrast in the result allows us to assume why StyleGAN-XL has\n        lower FID and KID but still does not produce realistic images for\n        humans.\n      </p>\n\n      <h3 className=\"pb-2\">\n        Evaluate outputs of StyleGAN2, StyleGAN3, and StyleGAN-XL\n      </h3>\n      <Image src={userstudyimages} className=\"class-img-top px-5\" alt=\"d2\" fluid />\n      <h4 className=\"pb-2 px-5\">Quantitative metrics</h4>\n      <p className=\"px-5\">\n        Three metrics calculate each model's performance in generating realistic\n        images: FID, KID, and P&R, where P&R breaks into correlated Precision\n        and Recall. The below table presents the mean and standard deviation of\n        each score for each model that is calculated ten times. The results show\n        that StyleGAN-XL is the best, followed by StyleGAN2 and StyleGAN3 based\n        on FID and KID. However, StyleGAN3 is the best model in terms of P&R as\n        it has the highest summation of Precision and Recall score (1.19605). If\n        observing Precision and Recall metrics separately, StyleGAN2 is the best\n        model for generating realistic images (the greatest Precision score)\n        while being the worst model for developing diverse images (the lowest\n        Recall score).\n      </p>\n      <h4 className=\"pb-2 px-5\">Qualitative metrics</h4>\n      <p className=\"px-5\">\n        The full results as a spreadsheet are at{\" \"}\n        <Link to=\"https://bit.ly/3dJgJao\">ggsheet</Link>. The summarized scores\n        are being processed in the jupyter notebook, which can be found in our{\" \"}\n        <Link to=\"https://bit.ly/3D4jQED\">GitHub</Link> page for this project.\n        The notebook contains basic logic to distinguish the answers to each\n        model's scoring as the numerical results. The below table provides the\n        summary statistics for each model on how well they can generate\n        synthetic images based on the user's perception. Of 1080 possible\n        answers (90 participants x 12) for each quality and diversity question,\n        StyleGAN2 receives the greatest attention for its realistic output\n        (44.81%), followed by StyleGANXL and StyleGAN3, whose scores are almost\n        the same. Similarly, participants selected results from the StyleGAN2\n        model to have more variety than the rest.\n      </p>\n\n      <div className=\"px-5\">\n        <Carousel prevLabel nextLabel interval=\"2000\">\n          <Carousel.Item>\n            <img className=\"d-block w-100 \" src={tb1} alt=\"First slide\" fluid />\n          </Carousel.Item>\n          <Carousel.Item>\n            <img\n              className=\"d-block w-100 \"\n              src={tb2}\n              alt=\"Second slide\"\n              fluid\n            />\n          </Carousel.Item>\n        </Carousel>\n      </div>\n\n      <h3 className=\"pb-2\">Result</h3>\n      <p>\n        The results from qualitative measurement turn out to be similar to only\n        the Precision metric. About 45\\% of participants selected images from\n        StyleGAN2 as the most realistic images, and the highest Precision score\n        of 0.68 goes to StyleGAN2. StyleGAN2 is also the most diverse model for\n        participants while being the least varied model according to the P&R\n        metric. This remarkable fact shows that most users assume that the more\n        realistic the model produces images, the more diverse the model can\n        create fake human faces. It is worth pointing out that the results from\n        the user study will follow the P&R metric if the diversity results of\n        StyleGAN3 in above table gets slightly more user votes (3%). While\n        StyleGAN-XL has the lowest score among FID and KID, it does not look\n        realistic from the human perspective. Almost half of the participants\n        from the user study did not choose outputs from StyleGAN-XL as the more\n        lifelike images. Most of the comments from userstudy mentioned that\n        their skins look too smooth and the position of facial features are\n        abnormal such as the angle of smile and face do not get along.\n      </p>\n      <h3 className=\"pb-2\">Further work</h3>\n      <p>\n        There are two aspects for the further investigation of this project.\n        Firstly, regarding the unmatch results between the metrics and user\n        study, StyleGAN-XL has a better FID and KID but is ranked worse by\n        participants. The assumption of this result would be because of\n        pre-trained class embeddings. Adding this configuration will allow the\n        model to train on a larger dataset as it generates more diverse samples\n        per class. Using pre-trained networks trained in other datasets like\n        Imagenet may allow the StyleGAN-XL model to reduce the FID and KID\n        scores; the activations of those pre-trained models can affect the\n        metrics. Secondly, the user study results could be more standardised and\n        fair. The current version of the user study has three images to compare\n        with another three. By comparing six syntheses, some users cannot truly\n        judge the diversity of each model. Therefore, one of the further works\n        should be redesigning the user study so that it could collect unbias\n        data from humans. Also, the images shown in the current version of the\n        user study are randomly selected and laid out in each question. It would\n        be better to collect the same image types from each model, for instance,\n        having the same number of men's faces, a person wearing glasses, and\n        children.\n      </p>\n      <div></div>\n    </div>\n\n    <Footer></Footer>\n  </div>\n)\n\nexport default cookBurnPage\n"],"names":["_excluded","Image","_ref","ref","bsPrefix","className","fluid","rounded","roundedCircle","thumbnail","props","classes","displayName","defaultProps","Header","style","margin","maxWidth","paddingBottom","src","userstudyimages","alt","Link","to","Carousel","prevLabel","nextLabel","interval","tb1","tb2","Footer"],"sourceRoot":""}